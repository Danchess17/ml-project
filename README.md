# Блочно-ортогональная параметризация сверток в задаче классификации изображений

## Постановка задачи
Цель - ввести эффективную блочно-ортогональную параметризацию слоя свертки и внедрить её в базовые модели сверточных сетей, сравнить с базовой моделью.
Основная задача - уменьшить количество обучаемых параметров, при этом не сильно теряя в точности.

### Формат входных и выходных данных
Входные данные: RGB-изображение размером 32х32

Выходные данные: номер класса

### Метрики
- Accuracy 
- ROC-AUC
- Количество обучаемых параметров

Количество обучаемых параметров по построению будет меньше.
Планирую получить значения Accuracy сопоставимыми с базовой моделью, она должна быть не хуже, чем на 3%, гипотеза состоит в том, что ортогональная параметризация обладает большей обобщающей способностью, поэтому гипотетически есть шанс получить бОльший Accuracy при меньшем количестве параметров.

### Валидация
Датасет уже разделен на 50000 тренировочных изображений и 10000 тестовых. Размер валидации - это гиперпараметр, настраиваемый в файле конфигурации эксперимента (по дефолту 10% рандомно выбранных изображений из тренировочных данных, то есть 5000 изображений). Соответственно, тип валидации - Hold-Out Validation. Итоговое разделение для дефолтного значения параметра: 45000/5000/10000. 

### Данные
CIFAR-10 (возможно, еще CIFAR-100).
В обоих датасетах 60000 RGB-изображений размера 32х32
Ссылка на описание обоих датасетов: https://www.cs.toronto.edu/~kriz/cifar.html

## Моделирование
### Бейзлайн
Resnet-18 с небольшим видоизменением под CIFAR-10:
- ядро первого сверточного слоя теперь 3x3, а не 7x7 как в дефолтной архитектуре 
- убран MaxPooling слой после первого сверточного слоя.
Это дает прирост в +2% на CIFAR-10 по сравнению с обычной Resnet-18, поэтому будем использовать ее как бейзлайн под CIFAR-10. 
### Основная модель
Тот же самый бейзлайн (Resnet-18 с видоизменениями), только уже с блочно-ортогональной параметризацией сверток.
 На самом деле, эффективная архитектура слоев с блочно-ортогональными свертками на самом деле в точности неизвестна. Задача как раз и состоит в том, чтобы выявить более легковесные аналоги известных базовых моделей с наименьшей потерей точности. Гиперпараметры, связанные с архитектурой модели, будут описаны ниже в файле конфигурации.

### Внедрение
Модель будет использоваться как пакет. 
Для реализации новой параметризации свертки и соответственно новой архитектуры модели будет использоваться Pytorch, для обучения будет использоваться Pytorch Lightning.
Для трекинга экспериментов и логирования будут использоваться MLFlow/W&B, для визуализации метрик обучения - Tensorboard.
В режиме CLI можно запускать, редактировать и сохранять наилучшие конфигурации модели с помощью Hydra.

## Технические детали работы с проектом 
### Setup
- conda activate project-env
- pip install uv 
- uv sync / uv pip install -e .[dev]
- uv run pre-commit install
- uv run pre-commit run -a

### Method
Так как задача - сжать модель без потери точности, то будем обучать в сверточном слое не все c_out * c_in * k * k параметров, а только часть из них. Разворачиваем слой в матрицу размера c_out x (c_in * k * k). Хотим поддерживать ортогональность строк. Для этого можно сконкатенировать несколько квадратных ортогональных матриц размера c_out x c_out друг с другом, пока нам позволяет количество столбцов (c_in * k * k), пусть таких матриц будет num_matrices, их максимальное возможное количество - (c_in * k * k) // c_out. Так как возможен остаток от деления, то мы составляем матрицу нулей рамзера c_out x (с_in * k * k - c_out * num_matrices) и конкатенируем ее слева - эти параметры необучаемы. Соответственно, обучаемы только эти num_matrices ортогональных матриц в слое. Рассмотрим одну из них. В них мы тоже используем не все параметры, будем делать каждую из них блочно-ортогональной, а не просто ортогональной, то есть на диагонали будут стоять ортогональные матрицы-блоки одинаковых размеров, соответственно, таких блоков будем r в матрице (если r = 1,  то блок один, то есть матрица просто ортогональная). Соответственно, размер блока равен c_out / r. Гиперпараметр r един для всех num_matrices матриц, то есть количество блоков в каждой из них одинаковое. Соответственно, при r > 1 появляются необучаемые нули, и это снижает размер модели.

### Train 
Есть набор гиперпараметров, который описан в файле config.yaml и который можно менять через CLI благодаря Hydra:
- data_module - блок гиперпараметров, отвечающий за подготовку данных для обучения, он состоит из:
0. data_dir - названия папки с датасетом (default: data)
1. batch_size - размер батча (default: 128)
2. num_workers - количество воркеров для параллельного обучения (default: 4)
3. subset_fraction - размер доли тренировочного датасета, на котором хотим обучаться (default: 1.0 - весь тренировочный датасет, 50000 изображений). Нужен для сравнения модели с бейзлайном в условиях малых размеров обучающей выборки.
4. valid_size - размер валидационной выборки, доля берется от subset_fraction * 50000 изображений (default: 0.1 - при subset_fraction = 1.0 получаем 5000 изобраений).

- learning_rate - шаг оптимизатора (default: 0.001)
- max_epochs - максимальное количество эпох обучения (default: 20)
- baseline_name - имя базовой модели, с которой проводятся эксперименты (default: Resnet-18). Гиперпараметр на случай масштабирования проекта.
- model - блок гиперпараметров, связанных с самим методом параметризации слоев нейронной сети, он состоит из:
1. odk_layers - список блоков (у Resnet-18 их всего 4), к которым применяется метод (default: [3, 4] - два последних).
2. num_matrices - количество матриц для конкатенации в 2д-представлении сверточного слоя (default: 1). Максиимальное возможное значение (c_in * k * k) // c_out.
2. r - количество блоков внутри одной такой матрицы, гиперпараметр един для всех матриц (default: 1 - просто ортогональная матрица) 

### Infer


